{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c34ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c188a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embed the user query using the same MiniLM model\n",
    "API_URL = \"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-MiniLM-L6-v2/pipeline/feature-extraction\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['HUGGINGFACE_API_TOKEN']}\"}\n",
    "user_query = \"Tell me his education?\"\n",
    "\n",
    "\n",
    "def embedding_agent(user_query):\n",
    "\n",
    "    payload = {\"inputs\": [user_query]}\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    query_embedding = response.json()[0]\n",
    "\n",
    "    # 2. Connect to PostgreSQL and perform similarity search\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"vectordb\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\",\n",
    "        host=\"localhost\",\n",
    "        port=5432\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    register_vector(conn)\n",
    "\n",
    "    # 3. Retrieve top 3 most similar document chunks\n",
    "    cur.execute(\n",
    "        \"SELECT content FROM documents ORDER BY embedding <-> %s::vector LIMIT 3\",\n",
    "        (query_embedding,)\n",
    "    )\n",
    "    results = cur.fetchall()\n",
    "\n",
    "    # 4. Print the results\n",
    "    # for idx, (content,) in enumerate(results):\n",
    "    #     print(f\"Result {idx+1}: {content[:200]}...\")  # Print first 200 chars\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e8f90",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'Client'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     results: List[\u001b[38;5;28mstr\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Initialize Gemini client once (recommended)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m client = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m(api_key=os.environ.get(\u001b[33m'\u001b[39m\u001b[33mGEMINI_API_KEY\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     18\u001b[39m app.add_middleware(\n\u001b[32m     19\u001b[39m     CORSMiddleware,\n\u001b[32m     20\u001b[39m     allow_origins=[\u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:5173\u001b[39m\u001b[33m\"\u001b[39m],  \u001b[38;5;66;03m# your frontend origin\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     allow_headers=[\u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@app\u001b[39m.post(\u001b[33m\"\u001b[39m\u001b[33m/chatbot\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magent\u001b[39m(request: ChatbotRequest):\n",
      "\u001b[31mAttributeError\u001b[39m: module 'google.generativeai' has no attribute 'Client'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "import google.generativeai as genai\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "# Configure the Gemini client\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")  # or gemini-pro, etc.\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:5173\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class ChatbotRequest(BaseModel):\n",
    "    query: str\n",
    "    results: List[str]\n",
    "\n",
    "@app.post(\"/chatbot\")\n",
    "async def agent(request: ChatbotRequest):\n",
    "    user_query = request.query\n",
    "    results = request.results\n",
    "\n",
    "    # This assumes you have embedding_agent defined somewhere\n",
    "    embedding_query = embedding_agent(user_query)\n",
    "    context_str = \"\\n\\n\".join([content for (content,) in embedding_query])\n",
    "\n",
    "    embedding_results = []\n",
    "    for result in results:\n",
    "        embedding_results += embedding_agent(result)\n",
    "    context_str += \"\\n\\n\".join([content for (content,) in embedding_results])\n",
    "\n",
    "    print(f\"Context:{ context_str}\\n Question:{ user_query}\\n \")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Answer using context and known facts only. If uncertain, reply: 'I don't know based on the provided information.' Be concise within 50 words. Include summary or advice if relevant.\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question:\n",
    "{user_query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return {\"response\": response.text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded30a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
